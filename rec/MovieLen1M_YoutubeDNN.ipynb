{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtox72csOQUN"
   },
   "source": [
    "# DeepMatch 样例代码\n",
    "- https://github.com/shenweichen/DeepMatch\n",
    "- https://deepmatch.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTWHz-heMkyw"
   },
   "source": [
    "# 下载movielens-1M数据 安装依赖包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTl6d6jO1oqf"
   },
   "outputs": [],
   "source": [
    "! wget http://files.grouplens.org/datasets/movielens/ml-1m.zip -O ./ml-1m.zip\n",
    "! unzip -o ml-1m.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9UxNHuPMuW2"
   },
   "source": [
    "# 导入需要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "C_ZR6gzp1E2N"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from deepctr.feature_column import SparseFeat, VarLenSparseFeat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from deepmatch.models import *\n",
    "from deepmatch.utils import sampledsoftmaxloss, NegativeSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLenDataProcessor:\n",
    "    def __init__(self, seq_max_len=50, negsample=0):\n",
    "        self.seq_max_len = seq_max_len\n",
    "        self.negsample = negsample\n",
    "\n",
    "    def gen_data_set(self, data):\n",
    "        \"\"\"生成训练集和测试集\"\"\"\n",
    "        data.sort_values(\"timestamp\", inplace=True)\n",
    "\n",
    "        # 获取唯一的电影ID和电影ID到类型的映射\n",
    "        item_ids = data['movie_id'].unique()\n",
    "        item_id_genres_map = dict(zip(data['movie_id'].values, data['genres'].values))\n",
    "\n",
    "        train_set, test_set = [], []\n",
    "        for reviewerID, hist in tqdm(data.groupby('user_id')):\n",
    "            pos_list = hist['movie_id'].tolist()\n",
    "            genres_list = hist['genres'].tolist()\n",
    "            rating_list = hist['rating'].tolist()\n",
    "\n",
    "            neg_list = self.generate_neg_samples(pos_list, item_ids) if self.negsample > 0 else []\n",
    "            # 为当前用户生成训练和测试集\n",
    "            self.append_train_and_test_sets(train_set, test_set, reviewerID, pos_list, genres_list,\n",
    "                                            rating_list, neg_list, item_id_genres_map)\n",
    "        # 打乱训练和测试集\n",
    "        random.shuffle(train_set)\n",
    "        random.shuffle(test_set)\n",
    "\n",
    "        print(f'Train set entry size: {len(train_set[0])}, Test set entry size: {len(test_set[0])}')\n",
    "\n",
    "        return train_set, test_set\n",
    "\n",
    "    def generate_neg_samples(self, pos_list, item_ids):\n",
    "        \"\"\"生成负样本\"\"\"\n",
    "        candidate_set = list(set(item_ids) - set(pos_list))\n",
    "        return np.random.choice(candidate_set, size=len(pos_list) * self.negsample, replace=True)\n",
    "\n",
    "    def append_train_and_test_sets(self, train_set, test_set, reviewerID, pos_list, genres_list,\n",
    "                                   rating_list, neg_list, item_id_genres_map):\n",
    "        \"\"\"生成训练集和测试集条目并添加到相应集合\"\"\"\n",
    "        for i in range(1, len(pos_list)):\n",
    "            hist = pos_list[:i]\n",
    "            genres_hist = genres_list[:i]\n",
    "            seq_len = min(i, self.seq_max_len)\n",
    "\n",
    "            if i != len(pos_list) - 1:\n",
    "                train_set.append(self.create_entry(\n",
    "                    reviewerID, pos_list, genres_list, rating_list, hist, genres_hist, seq_len, i, 1))\n",
    "\n",
    "                for negi in range(self.negsample):\n",
    "                    neg_id = neg_list[i * self.negsample + negi]\n",
    "                    neg_genres = item_id_genres_map[neg_id]\n",
    "                    train_set.append(self.create_entry(reviewerID, neg_list, genres_list, rating_list, hist,\n",
    "                                                       genres_hist, seq_len, i, 0, neg_genres))\n",
    "            else:\n",
    "                test_set.append(self.create_entry(\n",
    "                    reviewerID, pos_list, genres_list, rating_list, hist, genres_hist, seq_len, i, 1))\n",
    "\n",
    "    @staticmethod\n",
    "    def create_entry(reviewerID, mov_list, genres_list, rating_list, hist, genres_hist, seq_len, index, label, genres_id=None):\n",
    "        \"\"\"生成条目\"\"\"\n",
    "        if genres_id is None:\n",
    "            genres_id = genres_list[index]\n",
    "        return (\n",
    "            reviewerID,\n",
    "            mov_list[index],\n",
    "            label,\n",
    "            hist[::-1][:seq_len],\n",
    "            seq_len,\n",
    "            genres_hist[::-1][:seq_len],\n",
    "            genres_id,\n",
    "            rating_list[index]\n",
    "        )\n",
    "\n",
    "    def gen_model_input(self, train_set, user_profile):\n",
    "        \"\"\"生成模型输入数据和标签\"\"\"\n",
    "        train_model_input = self.extract_features(train_set)\n",
    "        train_model_input.update(self.add_user_profile_features(train_model_input['user_id'], user_profile))\n",
    "        train_label = np.array([line[2] for line in train_set])\n",
    "        return train_model_input, train_label\n",
    "\n",
    "    def extract_features(self, train_set):\n",
    "        \"\"\"从训练集提取用户和电影ID、历史记录、电影类型等特征\"\"\"\n",
    "        train_uid = np.array([line[0] for line in train_set])\n",
    "        train_iid = np.array([line[1] for line in train_set])\n",
    "        train_seq = [line[3] for line in train_set]\n",
    "        train_hist_len = np.array([line[4] for line in train_set])\n",
    "        train_seq_genres = [line[5] for line in train_set]\n",
    "        train_genres = np.array([line[6] for line in train_set])\n",
    "\n",
    "        train_seq_pad = pad_sequences(train_seq, maxlen=self.seq_max_len, padding='post', truncating='post', value=0)\n",
    "        train_seq_genres_pad = pad_sequences(train_seq_genres, maxlen=self.seq_max_len, padding='post', truncating='post', value=0)\n",
    "\n",
    "        return {\n",
    "            \"user_id\": train_uid,\n",
    "            \"movie_id\": train_iid,\n",
    "            \"hist_movie_id\": train_seq_pad,\n",
    "            \"hist_genres\": train_seq_genres_pad,\n",
    "            \"hist_len\": train_hist_len,\n",
    "            \"genres\": train_genres\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def add_user_profile_features(user_ids, user_profile):\n",
    "        \"\"\"根据用户ID添加用户特征\"\"\"\n",
    "        user_features = {}\n",
    "        for feature in [\"gender\", \"age\", \"occupation\", \"zip\"]:\n",
    "            user_features[feature] = user_profile.loc[user_ids][feature].values\n",
    "        return user_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQq6O9XAMzPF"
   },
   "source": [
    "# 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lcO29zFb21Od",
    "outputId": "bfeed1ac-99f2-425f-dda6-10b83be721fe"
   },
   "outputs": [],
   "source": [
    "class MoiveLenDataLoader:\n",
    "    def __init__(self, data_path=\"./\"):\n",
    "        self.data_path = data_path\n",
    "        self.user_cols = ['user_id', 'gender', 'age', 'occupation', 'zip']\n",
    "        self.rating_cols = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "        self.movie_cols = ['movie_id', 'title', 'genres']\n",
    "\n",
    "    def load_users(self):\n",
    "        \"\"\"加载用户数据\"\"\"\n",
    "        user_file = f\"{self.data_path}ml-1m/users.dat\"\n",
    "        return pd.read_csv(user_file, sep='::', header=None, names=self.user_cols, engine='python')\n",
    "\n",
    "    def load_ratings(self):\n",
    "        \"\"\"加载评分数据\"\"\"\n",
    "        ratings_file = f\"{self.data_path}ml-1m/ratings.dat\"\n",
    "        return pd.read_csv(ratings_file, sep='::', header=None, names=self.rating_cols, engine='python')\n",
    "\n",
    "    def load_movies(self):\n",
    "        \"\"\"加载电影数据，并处理电影类型\"\"\"\n",
    "        movies_file = f\"{self.data_path}ml-1m/movies.dat\"\n",
    "        movies = pd.read_csv(movies_file, sep='::', header=None, names=self.movie_cols, encoding=\"unicode_escape\", engine='python')\n",
    "        movies['genres'] = movies['genres'].map(lambda x: x.split('|')[0])\n",
    "        return movies\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"加载并合并所有数据\"\"\"\n",
    "        users = self.load_users()\n",
    "        ratings = self.load_ratings()\n",
    "        movies = self.load_movies()\n",
    "\n",
    "        data = pd.merge(ratings, movies, on='movie_id')\n",
    "        data = pd.merge(data, users, on='user_id')\n",
    "        return data\n",
    "\n",
    "# 使用示例\n",
    "data_loader = MoiveLenDataLoader(data_path=\"./\")\n",
    "data = data_loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "      <td>One Flew Over the Cuckoo's Nest (1975)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "      <td>James and the Giant Peach (1996)</td>\n",
       "      <td>Animation</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "      <td>My Fair Lady (1964)</td>\n",
       "      <td>Musical</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "      <td>Erin Brockovich (2000)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "      <td>Bug's Life, A (1998)</td>\n",
       "      <td>Animation</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating  timestamp  \\\n",
       "0        1      1193       5  978300760   \n",
       "1        1       661       3  978302109   \n",
       "2        1       914       3  978301968   \n",
       "3        1      3408       4  978300275   \n",
       "4        1      2355       5  978824291   \n",
       "\n",
       "                                    title     genres gender  age  occupation  \\\n",
       "0  One Flew Over the Cuckoo's Nest (1975)      Drama      F    1          10   \n",
       "1        James and the Giant Peach (1996)  Animation      F    1          10   \n",
       "2                     My Fair Lady (1964)    Musical      F    1          10   \n",
       "3                  Erin Brockovich (2000)      Drama      F    1          10   \n",
       "4                    Bug's Life, A (1998)  Animation      F    1          10   \n",
       "\n",
       "     zip  \n",
       "0  48067  \n",
       "1  48067  \n",
       "2  48067  \n",
       "3  48067  \n",
       "4  48067  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0yCWxQxM3se"
   },
   "source": [
    "# 构建特征列，训练模型，导出embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BMOvk_de2ML3",
    "outputId": "962afe1c-d387-4345-861f-e9b974a0b495"
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 50\n",
    "BATCH_SIZE = 1024\n",
    "negsample = 0\n",
    "\n",
    "data_processor = MovieLenDataProcessor(seq_max_len=SEQ_LEN, negsample=negsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6040/6040 [00:07<00:00, 844.38it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set entry size: 8, Test set entry size: 8\n"
     ]
    }
   ],
   "source": [
    "# 1.Label Encoding for sparse features,and process sequence features with `gen_date_set` and `gen_model_input`\n",
    "\n",
    "class FeatureProcessor:\n",
    "    def __init__(self, data, sparse_features):\n",
    "        self.data = data.copy()\n",
    "        self.sparse_features = sparse_features\n",
    "        self.feature_max_idx = {}\n",
    "\n",
    "    def label_encode_sparse_features(self):\n",
    "        \"\"\"对稀疏特征进行 Label Encoding 并生成特征最大索引值\"\"\"\n",
    "        for feature in self.sparse_features:\n",
    "            lbe = LabelEncoder()\n",
    "            self.data[feature] = lbe.fit_transform(self.data[feature]) + 1\n",
    "            self.feature_max_idx[feature] = self.data[feature].max() + 1\n",
    "            \n",
    "    def extract_user_item_profiles(self):\n",
    "        \"\"\"提取用户特征和物品特征\"\"\"\n",
    "        user_profile = self.data[[\"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]].drop_duplicates('user_id').set_index(\"user_id\")\n",
    "        item_profile = self.data[[\"movie_id\"]].drop_duplicates('movie_id')\n",
    "        return user_profile, item_profile\n",
    "\n",
    "    def generate_user_item_lists(self):\n",
    "        \"\"\"生成用户的物品交互列表\"\"\"\n",
    "        return self.data.groupby(\"user_id\")['movie_id'].apply(list)\n",
    "\n",
    "    def process_features(self):\n",
    "        \"\"\"对稀疏特征进行处理并提取用户和物品特征\"\"\"\n",
    "        self.label_encode_sparse_features()\n",
    "        user_profile, item_profile = self.extract_user_item_profiles()\n",
    "        user_item_list = self.generate_user_item_lists()\n",
    "        return user_profile, item_profile, user_item_list\n",
    "\n",
    "# 稀疏特征列表\n",
    "sparse_features = [\"movie_id\", \"user_id\",\n",
    "                    \"gender\", \"age\", \"occupation\", \"zip\", \"genres\"]\n",
    "\n",
    "# 创建特征处理器并处理特征\n",
    "feature_processor = FeatureProcessor(data, sparse_features)\n",
    "\n",
    "user_profile, item_profile, user_item_list = feature_processor.process_features()\n",
    "train_set, test_set = data_processor.gen_data_set(feature_processor.data)\n",
    "train_model_input, train_label = data_processor.gen_model_input(train_set, user_profile)\n",
    "test_model_input, test_label = data_processor.gen_model_input(test_set, user_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.count #unique features for each sparse field and generate feature config for sequence feature\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "class FeatureConfig:\n",
    "    def __init__(self, feature_max_idx, embedding_dim=32, seq_len=50):\n",
    "        self.feature_max_idx = feature_max_idx\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def get_user_feature_columns(self):\n",
    "        \"\"\"生成用户特征列配置\"\"\"\n",
    "        return [\n",
    "            SparseFeat('user_id', self.feature_max_idx['user_id'], 16),\n",
    "            SparseFeat(\"gender\", self.feature_max_idx['gender'], 16),\n",
    "            SparseFeat(\"age\", self.feature_max_idx['age'], 16),\n",
    "            SparseFeat(\"occupation\", self.feature_max_idx['occupation'], 16),\n",
    "            SparseFeat(\"zip\", self.feature_max_idx['zip'], 16),\n",
    "            VarLenSparseFeat(SparseFeat('hist_movie_id', self.feature_max_idx['movie_id'], self.embedding_dim,\n",
    "                                        embedding_name=\"movie_id\"), self.seq_len, 'mean', 'hist_len'),\n",
    "            VarLenSparseFeat(SparseFeat('hist_genres', self.feature_max_idx['genres'], self.embedding_dim,\n",
    "                                        embedding_name=\"genres\"), self.seq_len, 'mean', 'hist_len')\n",
    "        ]\n",
    "\n",
    "    def get_item_feature_columns(self):\n",
    "        \"\"\"生成物品特征列配置\"\"\"\n",
    "        return [SparseFeat('movie_id', self.feature_max_idx['movie_id'], self.embedding_dim)]\n",
    "\n",
    "class NegativeSamplerConfig:\n",
    "    def __init__(self, train_model_input, item_feature_columns):\n",
    "        self.train_counter = Counter(train_model_input['movie_id'])\n",
    "        self.item_count = [self.train_counter.get(i, 0) for i in range(item_feature_columns[0].vocabulary_size)]\n",
    "\n",
    "    def get_sampler_config(self, num_sampled=255):\n",
    "        \"\"\"生成负采样器配置\"\"\"\n",
    "        return NegativeSampler('frequency', num_sampled=num_sampled, item_name=\"movie_id\", item_count=self.item_count)\n",
    "    \n",
    "embedding_dim = 32\n",
    "feature_max_idx = feature_processor.feature_max_idx\n",
    "    \n",
    "# 创建 FeatureConfig 实例\n",
    "feature_config = FeatureConfig(feature_max_idx, embedding_dim, SEQ_LEN)\n",
    "user_feature_columns = feature_config.get_user_feature_columns()\n",
    "item_feature_columns = feature_config.get_item_feature_columns()\n",
    "\n",
    "# 创建 NegativeSamplerConfig 实例\n",
    "negative_sampler_config = NegativeSamplerConfig(train_model_input, item_feature_columns)\n",
    "sampler_config = negative_sampler_config.get_sampler_config(num_sampled=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "Train on 988129 samples\n",
      "Epoch 1/40\n",
      "988129/988129 [==============================] - 12s 12us/sample - loss: 6.1262\n",
      "Epoch 2/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 4.9452\n",
      "Epoch 3/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 4.6423\n",
      "Epoch 4/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 4.4823\n",
      "Epoch 5/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 4.3678\n",
      "Epoch 6/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 4.2796\n",
      "Epoch 7/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 4.2084\n",
      "Epoch 8/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 4.1540\n",
      "Epoch 9/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 4.1056\n",
      "Epoch 10/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 4.0663\n",
      "Epoch 11/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 4.0326\n",
      "Epoch 12/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 4.0017\n",
      "Epoch 13/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.9749\n",
      "Epoch 14/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.9521\n",
      "Epoch 15/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.9303\n",
      "Epoch 16/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.9117\n",
      "Epoch 17/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.8963\n",
      "Epoch 18/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.8814\n",
      "Epoch 19/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.8670\n",
      "Epoch 20/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.8557\n",
      "Epoch 21/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.8423\n",
      "Epoch 22/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.8326\n",
      "Epoch 23/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.8216\n",
      "Epoch 24/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.8133\n",
      "Epoch 25/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.8044\n",
      "Epoch 26/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.7975\n",
      "Epoch 27/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.7903\n",
      "Epoch 28/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.7835\n",
      "Epoch 29/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.7775\n",
      "Epoch 30/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.7720\n",
      "Epoch 31/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.7661\n",
      "Epoch 32/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.7619\n",
      "Epoch 33/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.7554\n",
      "Epoch 34/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.7512\n",
      "Epoch 35/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.7451\n",
      "Epoch 36/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.7428\n",
      "Epoch 37/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.7386\n",
      "Epoch 38/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.7350\n",
      "Epoch 39/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.7282\n",
      "Epoch 40/40\n",
      "988129/988129 [==============================] - 11s 11us/sample - loss: 3.7273\n"
     ]
    }
   ],
   "source": [
    "# 3.Define Model and train\n",
    "\n",
    "import tensorflow as tf\n",
    "if tf.__version__ >= '2.0.0':\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "else:\n",
    "    K.set_learning_phase(True)\n",
    "\n",
    "model = YoutubeDNN(user_feature_columns, item_feature_columns, user_dnn_hidden_units=(128,64, embedding_dim), sampler_config=sampler_config)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=sampledsoftmaxloss)\n",
    "\n",
    "history = model.fit(train_model_input, train_label,  # train_label,\n",
    "                    batch_size=BATCH_SIZE, epochs=40, verbose=1, validation_split=0.0, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6040, 32)\n",
      "(3706, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:2424: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    }
   ],
   "source": [
    "# 4. Generate user features for testing and full item features for retrieval\n",
    "test_user_model_input = test_model_input\n",
    "all_item_model_input = {\"movie_id\": item_profile['movie_id'].values,}\n",
    "\n",
    "user_embedding_model = Model(inputs=model.user_input, outputs=model.user_embedding)\n",
    "item_embedding_model = Model(inputs=model.item_input, outputs=model.item_embedding)\n",
    "\n",
    "user_embs = user_embedding_model.predict(test_user_model_input, batch_size=2 ** 12)\n",
    "# user_embs = user_embs[:, i, :]  # i in [0,k_max) if MIND\n",
    "item_embs = item_embedding_model.predict(all_item_model_input, batch_size=2 ** 12)\n",
    "\n",
    "print(user_embs.shape)\n",
    "print(item_embs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_G3KWslKmJo"
   },
   "source": [
    "# 使用faiss进行ANN查找并评估结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SvyQLNVKkcs"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6TY1l27iJU8U",
    "outputId": "5a8ccdd3-af70-4c48-b859-84c4befddfdd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6040it [00:00, 6461.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall 0.3564569536423841\n",
      "hit rate 0.3564569536423841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_true_label = {line[0]:[line[1]] for line in test_set}\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from deepmatch.utils import recall_N\n",
    "\n",
    "index = faiss.IndexFlatIP(embedding_dim)\n",
    "# faiss.normalize_L2(item_embs)\n",
    "index.add(item_embs)\n",
    "# faiss.normalize_L2(user_embs)\n",
    "D, I = index.search(np.ascontiguousarray(user_embs), 50)\n",
    "s = []\n",
    "hit = 0\n",
    "for i, uid in tqdm(enumerate(test_user_model_input['user_id'])):\n",
    "    try:\n",
    "        pred = [item_profile['movie_id'].values[x] for x in I[i]]\n",
    "        filter_item = None\n",
    "        recall_score = recall_N(test_true_label[uid], pred, N=50)\n",
    "        s.append(recall_score)\n",
    "        if test_true_label[uid] in pred:\n",
    "            hit += 1\n",
    "    except:\n",
    "        print(i)\n",
    "print(\"\")\n",
    "print(\"recall\", np.mean(s))\n",
    "print(\"hit rate\", hit / len(test_user_model_input['user_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "colab_MovieLen1M_YoutubeDNN.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
