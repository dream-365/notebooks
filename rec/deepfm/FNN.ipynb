{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22287076-c78b-445a-a28d-58faa7683434",
   "metadata": {},
   "source": [
    "FNN模型结合了因子分解机（FM）和深度神经网络（DNN）的优点。\n",
    "\n",
    "1. **Input Fields**：输入的特征被组织成若干“域”（Field），每个域包含了一些特征。\n",
    "2. **FM Pretraining**：首先通过因子分解机（FM）预训练得到特征嵌入（Embedding）。\n",
    "3. **Deep Neural Network**：然后将这些预训练得到的嵌入作为输入，传递给深度神经网络（DNN），通过若干隐层（Hidden Layers）和最终输出层（Output Layer）进行预测。\n",
    "\n",
    "### FNN模型实现\n",
    "\n",
    "为实现FNN，我们需要以下步骤：\n",
    "1. 利用FM进行预训练获取特征嵌入。\n",
    "2. 使用特征嵌入作为输入，构建深度神经网络进行训练和预测。\n",
    "\n",
    "### 代码解释\n",
    "\n",
    "1. **FM模型**：\n",
    "   - `FM`类用于预训练得到特征的嵌入向量（`V`）。\n",
    "   \n",
    "2. **FNN模型**：\n",
    "   - `FNN`类包括以下部分：\n",
    "     - 初始化DNN权重。\n",
    "     - 使用FM模型预训练特征嵌入。\n",
    "     - 利用预训练得到的嵌入向量进行DNN训练。\n",
    "   \n",
    "3. **预训练和训练**：\n",
    "   - `pretrain_fm`方法调用`FM`模型进行预训练，获得特征嵌入。\n",
    "   - `fit`方法用于训练整个模型，包括预训练和DNN训练。\n",
    "   \n",
    "4. **前向和后向传播**：\n",
    "   - `_forward_dnn`方法通过DNN层进行前向传播。\n",
    "   - `_backward_dnn`方法计算DNN的梯度，用于更新权重和偏置。\n",
    "\n",
    "5. **预测**：\n",
    "   - `predict`方法通过DNN进行预测，返回最终的预测结果。\n",
    "\n",
    "通过上述实现，FNN模型能够有效利用FM预训练得到的特征嵌入，同时捕捉特征之间的高阶交互，提高模型的预测性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95d15b1a-d421-49c2-a110-ae92a8ad0222",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (5,) and (50,10) not aligned: 5 (dim 0) != 50 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 145\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Train FNN model\u001b[39;00m\n\u001b[1;32m    144\u001b[0m fnn \u001b[38;5;241m=\u001b[39m FNN(n_features\u001b[38;5;241m=\u001b[39mnum_features, k\u001b[38;5;241m=\u001b[39membedding_dim, hidden_layers\u001b[38;5;241m=\u001b[39mhidden_layers, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m--> 145\u001b[0m \u001b[43mfnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m    148\u001b[0m prediction \u001b[38;5;241m=\u001b[39m fnn\u001b[38;5;241m.\u001b[39mpredict(X[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[4], line 78\u001b[0m, in \u001b[0;36mFNN.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs):\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m---> 78\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 81\u001b[0m, in \u001b[0;36mFNN._update_weights\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y):\n\u001b[0;32m---> 81\u001b[0m     dnn_output, dnn_gradients \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_dnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m dnn_output\n\u001b[1;32m     83\u001b[0m     error \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(prediction)\n",
      "Cell \u001b[0;32mIn[4], line 95\u001b[0m, in \u001b[0;36mFNN._forward_dnn\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     92\u001b[0m cache \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma0\u001b[39m\u001b[38;5;124m'\u001b[39m: a}\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layers)):\n\u001b[0;32m---> 95\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdnn_weights\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mW\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdnn_weights[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     96\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(z)\n\u001b[1;32m     97\u001b[0m     cache[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m a\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (5,) and (50,10) not aligned: 5 (dim 0) != 50 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class FM:\n",
    "    def __init__(self, n_features, k, learning_rate=0.01, n_epochs=10):\n",
    "        self.n_features = n_features\n",
    "        self.k = k\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "        # Initialize weights\n",
    "        self.w0 = 0\n",
    "        self.W = np.zeros(n_features)\n",
    "        self.V = np.random.normal(scale=0.01, size=(n_features, k))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for epoch in range(self.n_epochs):\n",
    "            for i in range(X.shape[0]):\n",
    "                self._update_weights(X[i], y[i])\n",
    "    \n",
    "    def _update_weights(self, x, y):\n",
    "        linear_output = self.w0 + np.dot(self.W, x)\n",
    "        interaction_output = 0.5 * np.sum((np.dot(x, self.V) ** 2) - np.dot(x ** 2, self.V ** 2))\n",
    "        prediction = linear_output + interaction_output\n",
    "        error = y - self.sigmoid(prediction)\n",
    "\n",
    "        self.w0 += self.learning_rate * error\n",
    "        self.W += self.learning_rate * error * x\n",
    "        for f in range(self.n_features):\n",
    "            self.V[f] += self.learning_rate * error * (np.dot(x, self.V) - x[f] * self.V[f]) * x[f]\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_output = self.w0 + np.dot(X, self.W)\n",
    "        interaction_output = 0.5 * np.sum((np.dot(X, self.V) ** 2) - np.dot(X ** 2, self.V ** 2), axis=1)\n",
    "        return linear_output + interaction_output\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        return self.V\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "class FNN:\n",
    "    def __init__(self, n_features, k, hidden_layers, learning_rate=0.01, n_epochs=10):\n",
    "        self.n_features = n_features\n",
    "        self.k = k\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "        self.dnn_weights = self._init_dnn_weights()\n",
    "\n",
    "    def _init_dnn_weights(self):\n",
    "        layer_sizes = [self.n_features * self.k] + self.hidden_layers\n",
    "        dnn_weights = {}\n",
    "\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            dnn_weights[f\"W{i+1}\"] = np.random.normal(scale=0.01, size=(layer_sizes[i], layer_sizes[i+1]))\n",
    "            dnn_weights[f\"b{i+1}\"] = np.zeros(layer_sizes[i+1])\n",
    "\n",
    "        # Output layer\n",
    "        dnn_weights[f\"W{len(layer_sizes)}\"] = np.random.normal(scale=0.01, size=(layer_sizes[-1], 1))\n",
    "        dnn_weights[f\"b{len(layer_sizes)}\"] = np.zeros(1)\n",
    "\n",
    "        return dnn_weights\n",
    "\n",
    "    def pretrain_fm(self, X, y):\n",
    "        fm = FM(n_features=self.n_features, k=self.k, learning_rate=self.learning_rate, n_epochs=self.n_epochs)\n",
    "        fm.fit(X, y)\n",
    "        self.V = fm.get_embeddings()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.pretrain_fm(X, y)\n",
    "        for epoch in range(self.n_epochs):\n",
    "            for i in range(X.shape[0]):\n",
    "                self._update_weights(X[i], y[i])\n",
    "\n",
    "    def _update_weights(self, x, y):\n",
    "        dnn_output, dnn_gradients = self._forward_dnn(x)\n",
    "        prediction = dnn_output\n",
    "        error = y - self.sigmoid(prediction)\n",
    "\n",
    "        for i in range(len(self.hidden_layers) + 1):\n",
    "            self.dnn_weights[f'W{i+1}'] += self.learning_rate * error * dnn_gradients[f'W{i+1}']\n",
    "            self.dnn_weights[f'b{i+1}'] += self.learning_rate * error * dnn_gradients[f'b{i+1}']\n",
    "\n",
    "    def _forward_dnn(self, x):\n",
    "        # Ensure the input embedding vector has the correct shape\n",
    "        a = x.dot(self.V).flatten()  # Shape: (n_features * k,)\n",
    "        cache = {'a0': a}\n",
    "\n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            z = np.dot(a, self.dnn_weights[f'W{i+1}']) + self.dnn_weights[f'b{i+1}']\n",
    "            a = self.sigmoid(z)\n",
    "            cache[f'a{i+1}'] = a\n",
    "            cache[f'z{i+1}'] = z\n",
    "\n",
    "        z = np.dot(a, self.dnn_weights[f'W{len(self.hidden_layers)+1}']) + self.dnn_weights[f'b{len(self.hidden_layers)+1}']\n",
    "        a = self.sigmoid(z)\n",
    "        cache[f'a{len(self.hidden_layers)+1}'] = a\n",
    "        cache[f'z{len(self.hidden_layers)+1}'] = z\n",
    "\n",
    "        gradients = self._backward_dnn(cache, x)\n",
    "        return a[0], gradients\n",
    "\n",
    "    def _backward_dnn(self, cache, x):\n",
    "        gradients = {}\n",
    "        L = len(self.hidden_layers)\n",
    "        a_last = cache[f'a{L+1}']\n",
    "        dz = a_last * (1 - a_last)\n",
    "\n",
    "        for i in reversed(range(1, L+2)):\n",
    "            a_prev = cache[f'a{i-1}']\n",
    "            gradients[f'W{i}'] = np.outer(a_prev, dz)\n",
    "            gradients[f'b{i}'] = dz\n",
    "            if i > 1:\n",
    "                da = dz.dot(self.dnn_weights[f'W{i}'].T)\n",
    "                dz = da * a_prev * (1 - a_prev)\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    def predict(self, x):\n",
    "        dnn_output, _ = self._forward_dnn(x)\n",
    "        return self.sigmoid(dnn_output)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Test the FNN model\n",
    "num_samples = 1000\n",
    "num_features = 10\n",
    "embedding_dim = 5\n",
    "hidden_layers = [10, 5]\n",
    "\n",
    "# Generate random dataset\n",
    "X = np.random.randint(2, size=(num_samples, num_features))\n",
    "y = np.random.randint(2, size=num_samples)\n",
    "\n",
    "# Train FNN model\n",
    "fnn = FNN(n_features=num_features, k=embedding_dim, hidden_layers=hidden_layers, learning_rate=0.01, n_epochs=10)\n",
    "fnn.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "prediction = fnn.predict(X[0])\n",
    "print(prediction)  # Print the first prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50511aff-d6ce-4cee-816b-d22c30ba90c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
